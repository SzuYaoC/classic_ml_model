{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81918b3d",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## 1. Definition\n",
    "Logistic regression is derived from the linear regression but it is a classification model and it's a supervised learning model. The main goal of logistic regress is to estimate the probability that an data instance belongs to a particular class. Built upon linear regression that predicts a numerical value, logistic regression transforms the output to fit between 0 and 1 using sigmoid function / logit function, making it sutiable for classification tasks.\n",
    "\n",
    "\n",
    "## 2. Core Idea\n",
    "Logistic regression assume the linearity between input features and output in the log-odds form. In other words, the input features relate linearly to the logit of target. Some other assumptions are: no multicollinearity, which means predictors should not be highly correlated with each other.\n",
    "\n",
    "\n",
    "## 3. Mechanism\n",
    "Logistic regression maps predictions into a probability between 0 and 1 through a sigmoid function:\n",
    "$$\\alpha(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "* The output $\\hat{p}$ >= 0.5, the model classifies the instances as class 1; otherwise, it's class 0.\n",
    "\n",
    "* Log-Odds: The linear component $z$ is equivalent to the log-odds. Log-odds = the logarithm of the odds of an event happening vs not happening. \n",
    "\n",
    "$$ odds = \\frac{p}{1-p}$$\n",
    "\n",
    "$$log-odds = log(\\frac{p}{1-p})$$\n",
    "\n",
    "\n",
    "## 4. Mathematical Details\n",
    "The goal of training is to find the optimal weights $w$ and bias $b$ that minimize the difference between the predicted probabilities $\\hat{p}$ and the actual class labels $y$.\n",
    "* Loss function: Logistic regeression uses the Binary Cross-Entropy Loss (or Log Loss) because it penzlizes confident incorrect predictions heavily, which is necessary for probabilities.\n",
    "\n",
    "$$J(w) = -[y\\log{(\\hat{y})} + (1-y)\\log{(1-\\hat{y})}] $$\n",
    "\n",
    "\n",
    "Total Cost J over all m examples:\n",
    "\n",
    "$$ J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}, y^{(i)})$$\n",
    "\n",
    "\n",
    "* Optimization: Itâ€™s a convex optimization problem, so it has a guaranteed global optimum. The weights are updated iteratively using an optimization algorithm like Gradient Descent or its variant (e.g. SGD, Adam). The gradient determines the direction to move in the weight space to minimize the loss.\n",
    "\n",
    "The update rules are derived by taking the **partial derivatives** of the cost function $J$ with respect to $w$ and $b$.\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})x_j^{(i)} $$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) $$\n",
    "\n",
    "## 5. Pros and Cons\n",
    "* Pros\n",
    "    * Works well for linearly separable dataset\n",
    "    * Highly interpretable (weights indicate features importance).\n",
    "    * Fast to train\n",
    "    * Provide well-calibrated probabilities\n",
    "* Cons:\n",
    "    * Cannot handle complex, non-linear decision boundaries.\n",
    "    * Performs poorly with highly correlated features unless regularized\n",
    "    * Decision boundary is strictly linear\n",
    "    * Not ideal when classes are highly imbalanced\n",
    "\n",
    "\n",
    "\n",
    "## 6. Production Consideration\n",
    "* Model is lightweight, very low inference latency... good for real-time systems\n",
    "* Monitor for feature drift because linear models degreade quickly\n",
    "* Coefficients must be immutable if used in live scoring pipeline\n",
    "* Ensure consistency in feature scaling during training and inference\n",
    "* Often used in ensembles or a first-stage model\n",
    "\n",
    "\n",
    "## 7. Other Variants\n",
    "\n",
    "* Multinomial Logistic Regression: Used for classification tasks with more than two classes. It generalizes the sigmoid function to the Softmax function.\n",
    "* L1/L2 Regularized Logistic Regression: Used to prevent overfitting by adding a penalty term to the loss function.\n",
    "* Its performance can be significantly improved by manually creating non-linear features (e.g., polynomial features or interaction terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bdabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "root = os.path.abspath(\"..\")\n",
    "sys.path.append(root)\n",
    "\n",
    "\n",
    "\n",
    "from src.logistic_regression import LogisticRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f282c381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset():\n",
    "    np.random.seed(42)\n",
    "    X_pos = np.random.randn(50,2) + np.array([2,2])\n",
    "    X_neg = np.random.randn(50,2) + np.array([-2,2])\n",
    "\n",
    "    X = np.vstack((X_pos, X_neg))\n",
    "    y = np.hstack((np.ones(50),np.zeros(50)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "X, y = create_dataset()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827fded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a1f177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({np.int64(0): 51, np.int64(1): 49})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ba749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
