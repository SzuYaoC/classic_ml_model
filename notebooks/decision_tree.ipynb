{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb67ef21",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Great for:\n",
    "* Retail: customer segmentation, churn prediction\n",
    "* Finance: credit risk scoring, fraud detection\n",
    "* E-commerce (Booking.com): content ranking, matching signals\n",
    "* Healthcare: interpretable diagnosis rules\n",
    "\n",
    "\n",
    "\n",
    "## 1. Definition\n",
    "Decision Tree is a supervised learning algorithm that models decisions as a tree-like structure of binary rules. It recursively splits data into smaller subsets to predict a target class or value. It solves the problem of non-linear classification and regression by breaking complex data into simple, interpretable 'if-then-else' logic.\n",
    "\n",
    "\n",
    "## 2. Core Idea\n",
    "Decision Tree assumes that simple, hierarchical rules are sufficient to describe complex relationships. It partitions the feature space by asking a sequence of if/else questions (split) that try to make the labels in each region as 'pure' as possible. Each split aims to create child nodes that are more homogeneous in the target variable than the parent node.\n",
    "\n",
    "\n",
    "## 3. Mechanism\n",
    "Decision Trees optimize impurity reduction.\n",
    "* Classification: minimize Gini impurity or Entropy \n",
    "* Regression: minimize variance (MSE)\n",
    "\n",
    "At each node, the tree selects best feature and best threshold that maximizes the information gain (impurity decrease).\n",
    "The process starts at the root with the full dataset, and for each feature, the algorithm evaluate all possible split threshold and compute impurity before and after the split. The process recursively repeats and stop when max depth reaches or impruity is smaller the defined threshold.\n",
    "\n",
    "The final prediction is assigned based on: the most common class (classification) or mean target value (regression).\n",
    "\n",
    "\n",
    "## 4. Mathematical Details and Training\n",
    "The training process is a Greedy Algorithm (e.g. CART, classification and regression trees). It iterates through every feature to find the split that maximizes data purity.\n",
    "\n",
    "1. Classification Criteria:\n",
    "* Gini Impurity (used in CART): Measures the likelihood of incorrect classification of a random element, minimizing\n",
    "Probability of misclassification if you randomly label a sample according to the class distribution in the node.  Gini impurity is an alternative to entropy for measuring the impurity of a dataset. It's computationally simpler than entropy. A Gini impurity of 0 indicates that all samples in the set belong to the same class.\n",
    "\n",
    "\n",
    "$$ G = 1 - \\sum_{i=1}^{C}(p_i)^2$$\n",
    "\n",
    "* Entropy / Information Gain (used in ID3 / C4.5): Measures disorder. We want to maximize the gain (reduce in entropy).\n",
    "\n",
    "> Entropy is a measure of the randomness or impurity in a set of data, and it quantifies the uncertainty in a collection of samples.\n",
    "\n",
    "    - A set with a mix of different classes has high entropy (high impurity)\n",
    "    - A set where all samples belong to the same class has low entropy (zero impurity)\n",
    "\n",
    "In simple terms, the formula calculates the weighted average of the information content of each class. A higher value indicates a higher degree of unpredictability and disorder in the data.\n",
    "\n",
    "$$H(S) = -\\sum_{i=1}^{C}p_i\\log_2(p_i)$$\n",
    "\n",
    "$C$: number of unique class\n",
    "\n",
    "$p_i$: The probability of samples that belong to the class i\n",
    "\n",
    "$log2(p_i)$: This part measure the information content of a class. The base 2 is used becauase information is measured in bits.\n",
    "\n",
    "$-\\sum$: The negative sign is necessary because the logarithm of a probability (a number between 0 and 1) is always negative, and entropy is defined as a non-negative value.\n",
    "\n",
    "<br>\n",
    "\n",
    "Information Gain is the primary metric used to select the best feature to split on at each step of the tree-building process. It measures the *reduction in entropy achieved by splitting the data* on a particular feature. The feature with the highest information gain is chosen for the split.\n",
    "\n",
    "**IG(S,A)**: This is the value we are trying to find. It represents the Information Gain achieved by splitting the dataset S on the feature A.\n",
    "$$IG(S,A) = H(S) - \\sum_{v \\in \\text{Values}{(A)}}\\frac{[S_v]}{|S|}; H(S_v)$$\n",
    "\n",
    "\n",
    "$S$: Original Set\n",
    "\n",
    "$A$: The feature being considered\n",
    "\n",
    "$Values(A)$: Possible values /thresholds for $A$\n",
    "\n",
    "$S_v$: the subset of $S$ where feature $A$ has value $v$.\n",
    "\n",
    "\n",
    "The model tries to choose splits that produce children with low impurity, i.e., clean class separation.\n",
    "\n",
    "<br><br>\n",
    "2. Regression:\n",
    "* Variance Reduction: It minimizes the Mean Squared Error (MSE) within the split. The tree stops growing when it hits max depth, min samples per leaft, or when a split creates no purity gain.\n",
    "\n",
    "\n",
    "\n",
    "## 5. Pros and Cons\n",
    "* Pros\n",
    "    * Interpretability: We can visualize the exact path taken to make a decision (good for banking / medical compliance).\n",
    "    * No preprocessing: Does not require feature scaling or dummy variables.\n",
    "    * Non-linear relationship\n",
    "    * Fast predictions\n",
    "    * Capture interactions automatically\n",
    "* Cons:\n",
    "    * Overfittingh: Trees tend to grow very deep and memorize noise, leading to high variance.\n",
    "    * Instability: A small change in the data can result in a complete different tree structure.\n",
    "    * Greedy splitting (locally optimal, not globally)\n",
    "    * Orthogonal Boundaries: Decision trees only split parallel to the axes ($X > 5$). They struggle with diagonal relationships (e.g., $X > Y$).\n",
    "\n",
    "\n",
    "## 6. Production Consideration\n",
    "* Pruning: In production, you never let a tree grow fully. You use Pre-pruning (setting max_depth or min_samples_split) or Post-pruning (cutting back weak branches) to generalize better.\n",
    "* Ensembling: A single decision tree is rarely used in high-performance production systems. They are almost always used as the building blocks for Random Forests (Bagging) or XGBoost/LightGBM (Boosting) to cure the instability and overfitting issues.\n",
    "* Inference Speed: Extremely fast O(depth). making them suitable for low-latency applications.\n",
    "\n",
    "\n",
    "\n",
    "## 7. Other variants\n",
    "* Bagging: Random Forest -> Avoid Overfitting, parallelizable.\n",
    "* Boosting: XGBoot -> Reduce bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98cecb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "root = os.path.abspath(\"..\")\n",
    "sys.path.append(root)\n",
    "\n",
    "\n",
    "from src.decision_tree import DecisionTree\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2031c97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa91dc1",
   "metadata": {},
   "source": [
    "## Case 1: Content ranking and search for hotel booking system\n",
    "\n",
    "### Business Problem\n",
    "Ranking System (via Decision Trees) is a predictive model that sorts a list of properties (hotels, apartments) for a specific user query by predicting the probability of a \"Conversion\" (Booking) based on hierarchical feature interactions. A search for \"Hotels in London\" returns 4,000 results. The system must order them so the most relevant 10 appear on the first screen.\n",
    "\n",
    "The existing heuristic ranking relied heavily on static signals (e.g., popularity, star rating) and didn’t capture personalization or property–user match quality. This resulted in:\n",
    "* Lower than expected CTR on search result pages\n",
    "* Weak alignment between user preferences and shown properties\n",
    "* Difficulty explaining ranking decisions to PMs and Legal/Compliance\n",
    "\n",
    "\n",
    "The specific goal was to increase the Conversion Rate (CVR) by at least 1% without increasing search latency beyond 50ms, ensuring we could better match property attributes to specific user contexts.\"\n",
    "\n",
    "\n",
    "### Data Problem\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Dataset and Feature Engineering\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39fed4d",
   "metadata": {},
   "source": [
    "## Case 2: Credit Risk Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ed109",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
