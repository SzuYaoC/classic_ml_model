{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8256da6",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Although Naive Bayes is best known for text classification, it is also heavily used in spam detection, search ranking, recommender systems, CTR prediction, fraud detection, medical diagnosis, anomaly detection, and even early computer vision pipelines. Anywhere you have high-dimensional sparse features or small datasets, Naive Bayes is a fast, interpretable, and surprisingly strong baseline.\n",
    "\n",
    "* Use case:\n",
    "    * Classifying user reviews (positive/negative) -> Sentiment analysis\n",
    "    * Click-through prediction\n",
    "    * Recommender Systems: Estimate probability that a user will like item B given they liked item A.\n",
    "        * Cold start problem\n",
    "        * Predict next product category\n",
    "    * Fraud & Rrik Modelling\n",
    "        * Credit risk scoring\n",
    "        * Likelihood of fraud\n",
    "        * Insurance claim fraud detection\n",
    "        * AML risk segmentation\n",
    "    * A/B Testing & Experimentation\n",
    "        * Bayesian bandits\n",
    "        * Probabilistic inference of user behaviour\n",
    "\n",
    "## 1. Definition\n",
    "Naive Bayes is a probabilistic classification algorithm based on Bayes' Theorem that applies a strong (naive) independence assumption between features to predict the class of a data point (Given feature X, what is the most likely class Y?)\n",
    "\n",
    "It's a generative model taht calculates the probability of a class given a set of features by counting frquencies in the training data. Naive Bayes solves the computational intractability of calculating full joint probabilities in high-dimensional data (e.g. text) where estimating interactions between every pair of features requires exponentially large datasets.\n",
    "\n",
    "## 2. Core Idea\n",
    "To resolve the spam or not spam in email classification problem, we need to calculate paired correlations between words. As vocabulary grows, the combinations become infinite. Therefore, we simply assume the text features are unrelated / independent events so that we can simply calculate their individual probabilities and multiply them.\n",
    "\n",
    "This assumption is where 'Naive' comes from. It simplifies the math enough to work surprisingly well for classification.\n",
    "\n",
    "## 3. Mechanism\n",
    "The model works by decomposing the posterior probability into three learnable components:\n",
    "* The Prior ($P(y)$): How common is this class in general?\n",
    "* The Likelihood ($P(X_i|y)$): If the email is Spam, what is the probability of seeing the word 'free'?\n",
    "* The Evidence ($P(x)$): TThe probability of the data itself (usually ignored during prediction as it is constant for all classes).\n",
    "\n",
    "Naive Bayes applies Bayes’ Theorem:\n",
    "$$P(X|Y) = \\frac{P(Y|X)P(Y)}{P(X)}$$\n",
    "\n",
    "where $P(B) != 0$\n",
    "\n",
    "The naive assumption allows us to factor: \n",
    "\n",
    "$$P(X|Y) = \\prod_{i=1}^d P(x_i | Y)$$\n",
    "\n",
    "Workflow:\n",
    "- Training: Simply count the occurrences of every feature for every class.\n",
    "- Inference: For a new data point, look up the probabilities for its features, multiply them by the prior, and pick the class with the highest score.\n",
    "\n",
    "## 4. Mathematical Details / Training\n",
    "Naive Bayes is distinct because it is not trained via gradient descent. It is trained via Maximum Likelihood Estimation (MLE).\n",
    "* The Naive Assumption:\n",
    "\n",
    "$$\\hat{y}=\\arg\\max_y P(Y=y)\\prod_{i=1}^d P(x_i |Y=y)$$\n",
    "\n",
    "Components:\n",
    "* Prior: $P(Y=y)$ .. Count how many times each class appears\n",
    "* Likelihood of features:\n",
    "    - Multinomial NB → counts (text)\n",
    "\t- Bernoulli NB → binary features\n",
    "\t- Gaussian NB → continuous features\n",
    "* Posterior: $P(Y|X)$\n",
    "\n",
    "Instead of a complex join distribution, we just multiply the individual probabilities of feature $x_i$.\n",
    "\n",
    "* Optimization:\n",
    "There is no \"loss function\" in the traditional sense (like MSE or Cross-Entropy) to minimize iteratively.\n",
    "The weights are closed-form solutions derived directly from frequency counts in the data.\n",
    "\n",
    "* Algorithmic Principle (Log-Sum-Exp): (Avoid underflow using logs)\n",
    "Since multiplying many small probabilities results in numerical underflow (numbers become too close to zero), we operate in log-space. Multiplication becomes addition:\n",
    "$$\\log(P(y|x)) \\propto \\log(P(y)) + \\sum \\log(P(x_i | y))$$\n",
    "\n",
    "\n",
    "## 5. Pros and Cons\n",
    "* Pros\n",
    "    * Speed: Training is O(N)... one pass over data; Inference is O(D) ..linear with number of features\n",
    "    * Small Data: Performs well even with small datasets because it has high bias but very low variance\n",
    "    * Works very well for text.\n",
    "    * Handles high dimensionality.\n",
    "    * Multiclass: Handles multiclass classification natively without needing 'One-vs-Rest' strategies.\n",
    "* Cons:\n",
    "    * The independence assumption: In domains where feature interaction is critical (e.g., image pixels or \"Not Good\" in sentiment analysis), it fails.\n",
    "    * Probability Calibration: While the ranking of classes is usually correct (Class A > Class B), the actual predicted probabilities are often inaccurate (pushing towards 0 or 1) due to the independence assumption violating reality.\n",
    "    * Performs poorly with correlated or non-Gaussian continuous features.\n",
    "    * Not suitable for complex decision boundaries.\n",
    "    * Zero Frequency Problem: If a word (e.g., \"Casino\") never appears in the Training Set, the probability becomes 0. If you multiply by 0, the whole prediction dies. (Solved via Laplace Smoothing).\n",
    "\n",
    "\n",
    "\n",
    "## 6. Production Consideration\n",
    "* Scalability: It is trivially parallelizable. You can count word frequencies on 10 different machines and sump them up. It is perfect for streaming data / online learning.\n",
    "* Latency: Idea for high-frequency/low-latency application, where you cannot afford the inference cost of a Transformer or a Deep Neural Net.\n",
    "* Model Compression: The model is just a lookup table of probabilities. It requires very little RAM compared to random forests or deep learning models.\n",
    "\n",
    "\n",
    "## 7. Other Variants\n",
    "* The zero frequency error - Laplace Smoothing\n",
    "    * Scenario: The word \"Bitcoin\" appears frequently in your Spam training data, but it has never appeared in your Ham training data.\n",
    "    * $$P(\\text{\"Bitcoin\"} | \\text{Ham}) = \\frac{\\text{Count of \"Bitcoin\" in Ham}}{\\text{Total Words in Ham}} = \\frac{0}{10,000} = 0$$\n",
    "    * e.g. \"Hey, did you see the news about Bitcoin?\" -> \n",
    "\n",
    "    $$P(\\text{Ham} | \\text{Email}) \\propto P(\\text{Ham}) \\times P(\\text{\"Hey\"}|Ham) \\times \\dots \\times \\mathbf{P(\\text{\"Bitcoin\"}|Ham)}$$$$P(\\text{Ham} | \\text{Email}) \\propto 0.5 \\times 0.05 \\times \\dots \\times \\mathbf{0} = \\mathbf{0}$$\n",
    "\n",
    "    * The result: The probability becomes absolute zero. The model becomes infinitely confident that this cannot be Ham, purely because of a lack of data.\n",
    "\n",
    "* Laplace Smoothing (also called Additive Smoothing) solves this by incorporating a prior belief: \"Every word is possible, even if we haven't seen it yet.\" Mechanically, we add a small \"pseudocount\" (usually 1) to every word in our vocabulary for every class.\n",
    "\n",
    "Standard Version:\n",
    "$$P(w_i | class) = \\frac{\\text{count}(w_i)}{\\text{total words in class}}$$\n",
    "\n",
    "Smoothed Version:\n",
    "$$P(w_i | class) = \\frac{\\text{count}(w_i) + \\alpha}{\\text{total words in class} + (\\alpha \\times V)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\text{count}(w_i)$: The actual times we saw the word.\n",
    "* $\\alpha$ (Alpha): The smoothing parameter\n",
    "    * If $\\alpha = 1$, it is called Laplace Smoothing.\n",
    "    * If 3$0 < \\alpha < 1$, it is called Lidstone Smoothing.\n",
    "* $V$: The size of the Vocabulary (total unique words in the dataset).5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3b6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "root = os.path.abspath(\"..\")\n",
    "\n",
    "sys.path.append(root)\n",
    "\n",
    "from src.naive_bayes import MultinomialNaiveBayes\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1a40c",
   "metadata": {},
   "source": [
    "## Case 1: A/B Testing \n",
    "\n",
    "### Business Problem:\n",
    "At hotel.com (mock scenario), we were running an A/B test to evaluate a new homepage layout intended to increase booking conversions.\n",
    "While the A/B test gave population-level uplift, leadership wanted:\n",
    "\n",
    "\t1.\tUser-level conversion probability estimates, and\n",
    "\t2.\tWhich user segments benefit most from Variant B.\n",
    "\n",
    "\n",
    "Because the dataset was high-dimensional and included many categorical features (country, device, referral source), We chose Multinomial Naive Bayes as a fast, baseline probabilistic model to predict the likelihood of conversion for each variant.\n",
    "\n",
    "The task is to\n",
    "1. Build a conversion probability model to predict likelihood of conversion under Variant A vs B.\n",
    "2. Use Naive Bayes to identify which segments show the largest uplift.\n",
    "3. Provide recommendations on whether we should roll out Variant B to all users or only targeted groups.\n",
    "\n",
    "\n",
    "### Data Problem\n",
    "The data reflects real A/B test features:\n",
    "* variant (A/B)\n",
    "* device_type (mobile/desktop/tablet)\n",
    "* country (3 example markets)\n",
    "* previous_bookings (0/1/2/3)\n",
    "* landing_page_views (count features)\n",
    "* conversion (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297384d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>device</th>\n",
       "      <th>country</th>\n",
       "      <th>prev_bookings</th>\n",
       "      <th>landing_views</th>\n",
       "      <th>conversion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>desktop</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>desktop</td>\n",
       "      <td>NL</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>desktop</td>\n",
       "      <td>NL</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>tablet</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>mobile</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variant   device country  prev_bookings  landing_views  conversion\n",
       "0       A  desktop      US              0              3           0\n",
       "1       B  desktop      NL              0              2           1\n",
       "2       B  desktop      NL              0              2           0\n",
       "3       B   tablet      US              1              1           1\n",
       "4       A   mobile      US              2              5           1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "N = 8000\n",
    "df = pd.DataFrame({\n",
    "    \"variant\": np.random.choice([\"A\", \"B\"], N, p=[0.5, 0.5]),\n",
    "    \"device\": np.random.choice([\"mobile\", \"desktop\", \"tablet\"], N),\n",
    "    \"country\": np.random.choice([\"NL\", \"US\", \"UK\"], N),\n",
    "    \"prev_bookings\": np.random.poisson(1.0, N),\n",
    "    \"landing_views\": np.random.poisson(3.0, N)\n",
    "})\n",
    "\n",
    "# Create conversion with some realistic pattern:\n",
    "df[\"conversion\"] = (\n",
    "    0.05\n",
    "    + 0.04 * (df[\"variant\"] == \"B\").astype(int)\n",
    "    + 0.03 * (df[\"device\"] == \"mobile\").astype(int)\n",
    "    + 0.02 * (df[\"country\"] == \"US\").astype(int)\n",
    "    + 0.01 * df[\"prev_bookings\"]\n",
    "    + np.random.normal(0, 0.02, N)\n",
    ")\n",
    "df[\"conversion\"] = (df[\"conversion\"] > 0.1).astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14ecf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Engineering\n",
    "df[\"variant_bin\"] = (df[\"variant\"] == \"B\").astype(int)\n",
    "X = pd.get_dummies(df[[\"device\", \"country\"]], drop_first=False)\n",
    "\n",
    "# Add count-based features\n",
    "X[\"variant\"] = df[\"variant_bin\"]\n",
    "X[\"prev_bookings\"] = df[\"prev_bookings\"]\n",
    "X[\"landing_views\"] = df[\"landing_views\"]\n",
    "X_numeric = X.astype(float)\n",
    "\n",
    "y = df[\"conversion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f309d2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.naive_bayes.MultinomialNaiveBayes at 0x123afdeb0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNaiveBayes(alpha=1.0)\n",
    "model.fit(X_numeric.values, y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b27135",
   "metadata": {},
   "source": [
    "* Predict Conversion Probability by Variant\n",
    "* Predict probability of conversion if user is exposed to Variant A vs B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f7b51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two hypothetical scenarios per user\n",
    "X_A = X.copy()\n",
    "X_A[\"variant\"] = 0  # forced version A\n",
    "\n",
    "X_B = X.copy()\n",
    "X_B[\"variant\"] = 1  # forced version B\n",
    "\n",
    "P_A = model.predict_proba(X_A.values)[:, 1]\n",
    "P_B = model.predict_proba(X_B.values)[:, 1]\n",
    "\n",
    "df[\"uplift\"] = P_B - P_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46b2836b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device\n",
       " desktop   -2.213409\n",
       " mobile    -2.213409\n",
       " tablet    -2.213409\n",
       " Name: uplift, dtype: object,\n",
       " country\n",
       " NL   -2.213409\n",
       " UK   -2.213409\n",
       " US   -2.213409\n",
       " Name: uplift, dtype: object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uplift_by_device = df.groupby(\"device\")[\"uplift\"].mean()\n",
    "uplift_by_country = df.groupby(\"country\")[\"uplift\"].mean()\n",
    "\n",
    "uplift_by_device, uplift_by_country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248c74d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0c47f5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd9dddb0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dc94286",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
