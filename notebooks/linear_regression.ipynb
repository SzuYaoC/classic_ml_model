{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae65a56",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## 1. Definition\n",
    "Linear regression is the most basic form of supervised learning used for regression tasks. The main goal of linear regression is to find the best-fitting straight line (or called hyperplane in higher dimensions) that describe the linear relationship between input features and the target. The model is linaer because it assumes that output is a linear combination of the inputs.\n",
    "\n",
    "\n",
    "## 2. Core Idea\n",
    "Linear regression answers the question: “How does Y change when X changes?”\n",
    "\n",
    "It assumes:\n",
    "\t•\teach feature contributes additively\n",
    "\t•\tthe relationship is roughly linear\n",
    "\n",
    "\n",
    "## 3. Mechanism\n",
    "The key process in Linear Regression is determining the \"best-fit\" line by minimizing the error between the predicted values and the actual observed values. This is achieved using the Ordinary Least Squares (OSL) methods.\n",
    "\n",
    "The linear equation can be expressed as: $\\hat{Y} = w_1X_2 + w_2X_2 +...+ w_nX_n + b$\n",
    "\n",
    "In vector form, this is written more concisely as: $\\hat{Y}=w^TX+b$\n",
    "\n",
    "The model predicts the target by taking a weighted sum of the input features. The weights ($\\mathbf{w}$) and the bias ($b$) are the parameters the model learns.\n",
    "\n",
    "\n",
    "* The \"best-fit\" line is the one that minimizes the sum of the squared vertical distances between the data points and the line itself. These vertical distances are called the residuals (or errors, $\\epsilon$).\n",
    "* Minimization: By squaring the residuals, the method heavily penalizes large errors and ensures that positive and negative errors don't cancel each other out.\n",
    "\n",
    "\n",
    "\n",
    "## 4. Mathematical Details and Training\n",
    "Training involves optimizing the weights ($\\mathbf{w}$) and bias ($b$) to minimize the error defined by the loss function.\n",
    "* Loss function: \n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m}\\sum_{i=1}^m{(\\hat{Y}_i - Y_i)}^2$$\n",
    "\n",
    "    *  m: number of instance\n",
    "    *  2m: (instead of m) to simplify the derivative calculation.\n",
    "\n",
    "\n",
    "* Optimization:\n",
    "    * 1. Method 1: Closed-form solution (Normal Equation): For smaller datasets, the optimal weights can be calculated directly without iteration using linear algbra: (Slow in high dimensions)\n",
    "    $$\\hat{w} = (X^TX)^{-1}X^TY$$\n",
    "    * 2. Method 2: Iterative Solution (Gradient Descent): : For larger datasets, weights are iteratively adjusted by taking steps proportional to the negative of the gradient of the MSE function. The Gradient tells us the direction of the steepest ascent, so we move in the opposite direction to minimize the loss.\n",
    "\n",
    "    $$\\frac{\\partial L}{\\partial w} = -\\frac{1}{n}X^T(y-\\hat{y})$$\n",
    "\n",
    "## 5. Pros and Cons\n",
    "* Pros:\n",
    "    * Highly interpretable: Weights show the influence of each feature\n",
    "    * Extremetly fast to train and deploy, event with large datasets\n",
    "    * Acts as a baseline for regression tasks.\n",
    "* Cons:\n",
    "    * Assumes a linear relationship, fails on complex, non-linear data\n",
    "    * Very sensitive to outliers\n",
    "    * Sensitive to multicollinearity\n",
    "\n",
    "\n",
    "## 6. Production Consideration\n",
    "* Must ensure consistent feature preprocessing (scaling, encoding)\n",
    "* MOnitor for data drift\n",
    "* Outliers can severely degrade performance\n",
    "\n",
    "\n",
    "\n",
    "## 7. Other variants\n",
    "* Polynomial RegressionsL Add polynomial terms ($X^2$ etc) to the linear equation to model non-linear relationships\n",
    "* Regularized Regression: Adds a penalty term to the MSE loss function to prevent overfitting e.g. L1, L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234b8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "root_path = os.path.abspath(\"..\")\n",
    "sys.path.append(root_path)\n",
    "\n",
    "from src.linear_regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe1e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_dataset(n_samples=100):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    X = np.linspace(0,10,n_samples).reshape(-1,1) # (n_sample, 1)\n",
    "    y = 3 * X[:,0]+5\n",
    "    return X,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcdeec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1) (100,)\n"
     ]
    }
   ],
   "source": [
    "X, y = create_dataset()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "522ae558",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd2b254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.29554883  2.63965406  2.98375929  3.32786452  3.67196975  4.01607498\n",
      "  4.36018021  4.70428544  5.04839067  5.3924959   5.73660113  6.08070636\n",
      "  6.42481159  6.76891682  7.11302205  7.45712728  7.80123251  8.14533774\n",
      "  8.48944297  8.8335482   9.17765342  9.52175865  9.86586388 10.20996911\n",
      " 10.55407434 10.89817957 11.2422848  11.58639003 11.93049526 12.27460049\n",
      " 12.61870572 12.96281095 13.30691618 13.65102141 13.99512664 14.33923187\n",
      " 14.6833371  15.02744233 15.37154756 15.71565278 16.05975801 16.40386324\n",
      " 16.74796847 17.0920737  17.43617893 17.78028416 18.12438939 18.46849462\n",
      " 18.81259985 19.15670508 19.50081031 19.84491554 20.18902077 20.533126\n",
      " 20.87723123 21.22133646 21.56544169 21.90954692 22.25365215 22.59775737\n",
      " 22.9418626  23.28596783 23.63007306 23.97417829 24.31828352 24.66238875\n",
      " 25.00649398 25.35059921 25.69470444 26.03880967 26.3829149  26.72702013\n",
      " 27.07112536 27.41523059 27.75933582 28.10344105 28.44754628 28.79165151\n",
      " 29.13575673 29.47986196 29.82396719 30.16807242 30.51217765 30.85628288\n",
      " 31.20038811 31.54449334 31.88859857 32.2327038  32.57680903 32.92091426\n",
      " 33.26501949 33.60912472 33.95322995 34.29733518 34.64144041 34.98554564\n",
      " 35.32965087 35.6737561  36.01786132 36.36196655]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d64f8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
