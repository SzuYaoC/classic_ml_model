{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "630c5d24",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "* Excellent when:\n",
    "\t* You need high accuracy without heavy tuning\n",
    "\t* Data is tabular, mixed-type, nonlinear\n",
    "\t* You want a strong baseline before trying boosting models\n",
    "\n",
    "* Use cases:\n",
    "\n",
    "\t* Financial risk modeling and credit scoring\n",
    "\t* Fraud detection / AML / anomaly detection\n",
    "\t* E-commerce: CTR prediction, ranking shortlist scoring\n",
    "\t* Booking.com: semantic match scoring, candidate generation models\n",
    "\t* Healthcare: interpretable yet powerful predictors\n",
    "\t* Insurance pricing and underwriting models\n",
    "\n",
    "\n",
    "## 1. Definition\n",
    "Random Forest is a supervised ensemble learning algorithm that builds a \"forest\" of many uncorrelated Decision Tress during training and then merges their outputs  to improve prediction accuracy and stability. It solves the problem of Overfitting and High Vairance in single Decision Trees by injecting randomness into both data samples and feature selection at each split\n",
    "\n",
    "## 2. Core Idea\n",
    "e need Random Forest because single decision trees are \"high variance\" models. If you change the training data slightly, a single decision tree might change its structure completely. It memorizes noise. However, by training many trees on slightly different data and average them, the random errors cancel out, leaving the true signal.\n",
    "\n",
    "## 3. Mechanism\n",
    "The workflow relies on two specific randomization techniques: Bagging and Feature Randomness.\n",
    "\n",
    "* **Bootstrapping (Bagging)**: For each tree, we create a new training set bhy sampling N examples from the original tree 'with replacement'. (some data points appear multiple times)\n",
    "\n",
    "* **Feature Selection**: When splitting a node, the tree is forced to choose the best split from a random subset of feature (e.g. $\\sqrt{\\text{total feature}}$), not all features. This ensures trees are diverse / decorrelated.\n",
    "\n",
    "* **Aggregation**:\n",
    "    * Classification: Majority Vote (hard voting).\n",
    "    * Regression: Averaging all tree outputs\n",
    "\n",
    "**Soft voting** is calculated by taking the average of the probabilities predicted by each individual model for every class: $P(class) = \\frac{\\text{count of class in leaf}}{\\text{total sample in leaf}}$\n",
    "\n",
    "\n",
    "\n",
    "## 4. Mathematical Details / Training\n",
    "The math focuses on Variance Reduction.\n",
    "1. Out-of-bag (OOB) Error: estimates generalization withou a validation set. Since ~1/3 of data is left out of each bootstrap sample, these 'unseen' points are used to calculate error during training, acting as a built in validation set.\n",
    "2. Feature Subspace:\n",
    "    * Classification: $m = \\sqrt{p}$ features\n",
    "    * Regression: $m = \\frac{p}{3}$ features\n",
    "\n",
    "3. Algorithmic Principle: If the correlation between trees is low, the variance of the ensemble decreases as the number of trees increases.\n",
    "\n",
    "\t* Lower correlation â†’ better variance reduction\n",
    "\t* Trees explore different parts of the feature space\n",
    "\n",
    "\n",
    "## 5. Pros and Cons\n",
    "* Pros\n",
    "    * Robust to outliners, noise and missing values\n",
    "    * Much less overfitting than a single Decision Tree\n",
    "    * Handles nonlinearities and interactions automatically\n",
    "    * No scaling needed: It can handle unscaled data and categroical feature well.\n",
    "    * Feature importance: Automatically provides a feature importance score (measuing how much a feature reduces impurity on average across all trees).\n",
    "* Cons:\n",
    "    * Less interpretable: You lose the clear interpretability of a single tree.\n",
    "    * Slow inference: To get a prediction, the computer must run the input through 100 + tree\n",
    "    * Model size: storing 500 deep trees can consume significant RAM space.\n",
    "    * Extrapolation: Random Forest cannot predict values outside the range of the training data. It is a \"lookup\" algorithm, not a trend-following one.\n",
    "    * Not ideal for very large datasets.\n",
    "    * Sparse data: It performs poorly on very high-dimensional, sparse data (like text / TFIDF) compared to linear models.\n",
    "\n",
    "## 6. Production Consideration\n",
    "* Parallel Training\n",
    "* Latency vs. Accuracy: In real-time apps, you might reduce the number of trees or limit max_depth to trade a small amount of accuracy ofr much faster inference.\n",
    "* Deployment: Models can be compiled into simpler languages (C++, Java) or converted to formats like ONNX to speed up the if-else logic traversals in production.\n",
    "\n",
    "\n",
    "## 7. Other variants?\n",
    "* Isolation Forest (iForest): This is an Unsupervised variant used for Anomaly Detection, not classification. It assumes that anomalies are few and different. Therefore, they are easier to isolate than normal points. \n",
    "    * If a data point ends up in a leaf node very quickly (short path length), it is likely an anomaly.\n",
    "    * If a data point requires many splits to isolate (deep path length), it is likely normal data.\n",
    "* Balanced Random Forest: This handles Imbalanced Data. Standard Random Forest is biased toward the majority class. If you bootstrap normally, some trees might not even see a fraud case.\n",
    "    * It down-samples the majority class for every tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6431852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "root = os.path.abspath(\"..\")\n",
    "sys.path.append(root)\n",
    "\n",
    "from src.random_forest import RandomForest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a03f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a098b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceeaa024",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForest(n_trees=5, max_depth=10, n_features=int(X.shape[1]))\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20dd0210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "acc = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(f\"Random Forest Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713e1bd",
   "metadata": {},
   "source": [
    "## Case 1: Financial Risk Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7b18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7180bd3f",
   "metadata": {},
   "source": [
    "## Case 2: Insurance Pricing and Underwriting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7281ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
